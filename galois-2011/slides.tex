\documentclass{beamer}
\usepackage{listings}
\usepackage{pgfpages}
\pgfpagesuselayout{4 on 1}[a4paper,border shrink=5mm,landscape]

\title{Faster persistent data structures through hashing}
\author{Johan Tibell\\johan.tibell@gmail.com}
\date{2011-02-15}

\begin{document}
\lstset{language=Haskell}

\frame{\titlepage}

\begin{frame}
  \frametitle{Motivating problem: Twitter data analys}

  ``I'm computing a communication graph from Twitter data and then
  scan it daily to allocate social capital to nodes behaving in a good
  karmic manner.  The graph is culled from 100 million tweets and has
  about 3 million nodes.''

  \bigskip
  We need a data structure that is
  \begin{itemize}
  \item fast when used with string keys, and
  \item doesn't use too much memory.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Persistent maps in Haskell}

  \begin{itemize}
  \item \lstinline!Data.Map! is the most commonly used map type.
  \item It's implemented using size balanced trees.
  \item Keys can be of any typs, as long as values of the type can be
    ordered.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Real world performance of Data.Map}

  \begin{itemize}
  \item Good in theory: no more than $O(\log n)$ comparisons.
  \item Not great in practice: up to $O(\log n)$ comparisons!
  \item Many common types are expensive to compare e.g
    \lstinline!String!, \lstinline!ByteString!, and \lstinline!Text!.
  \item Given a string of length $k$, we need $O(k*\log n)$
    comparisons to look up an entry.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Hash tables}
  \begin{itemize}
  \item Hash tables perform well with string keys: $O(k)$ amortized
    lookup time for strings of length $k$.
  \item However, we want persistent maps, not mutable hash tables.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Milan Straka's idea: Patricia trees as spare arrays}
  \begin{itemize}
  \item We can use hashing without using hash tables!
  \item A Patricia tree implements a persistent, sparse array.
  \item Patricia trees are as twice fast as size balanced trees, but
    only work with \lstinline!Int! keys.
  \item Use hashing to derive an \lstinline!Int! from an arbitrary
    key.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Implementation tricks}
  \begin{itemize}
  \item Patricia trees implement a spare, persistent array of size
    $2^{32}$ (or $2^{64}$).
  \item Hashing using this many buckets makes collisions rare: for
    $2^{24}$ entries we expect about 32,000 single collisions.
  \item Linked lists are a perfectly adequate collision resolution
    strategy.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{First attempt at an implementation}
  \begin{lstlisting}
-- Defined in the containers package.
data IntMap a
    = Nil
    | Tip {-# UNPACK #-} !Key a
    | Bin {-# UNPACK #-} !Prefix
          {-# UNPACK #-} !Mask
          !(IntMap a) !(IntMap a)

type Prefix = Int
type Mask   = Int
type Key    = Int

newtype HashMap k v = HashMap (IntMap [(k ,v)])
  \end{lstlisting}
\end{frame}


\begin{frame}[fragile]
  \frametitle{A more memory efficient implementation}
  \begin{lstlisting}
data HashMap k v
    = Nil
    | Tip {-# UNPACK #-} !Hash
          {-# UNPACK #-} !(FL.FullList k v)
    | Bin {-# UNPACK #-} !Prefix
          {-# UNPACK #-} !Mask
          !(HashMap k v) !(HashMap k v)

type Prefix = Int
type Mask   = Int
type Hash   = Int

data FullList k v = FL !k !v !(List k v)
data List k v = Nil | Cons !k !v !(List k v)
  \end{lstlisting}
\end{frame}

\begin{frame}
  \frametitle{Reducing the memory footprint}
  \begin{itemize}
  \item \lstinline!List k v! uses 2 fewer words per key/value pair
    than \lstinline![(k, v)]!.
  \item \lstinline!FullList! can be unpacked into the \lstinline!Tip!
    constructor as it's a product type, saving 2 more words.
  \item Always unpack word sized types, like \lstinline!Int!, unless
    you really need them to be lazy.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Benchmarks}

  Keys: $2^{12}$ random 8-byte \lstinline!ByteString!s

  \bigskip
  \begin{tabular}{|c|c|c|}
    \hline \textbf{Operation} & \textbf{Map} & \textbf{HashMap} \\
    \hline insert & 1.00 & 0.43 \\
    \hline lookup & 1.00 & 0.28 \\
    \hline
  \end{tabular}
\end{frame}

\begin{frame}
  \frametitle{Can we do better?}
  \begin{itemize}
  \item We still need to perform $O(min(W, \log n))$ \lstinline!Int!
    comparisons, where $W$ is the number of bits in a word.
  \item The memory overhead per key/value pair is still quite high.
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Borrowing from our neighbours}
  \begin{itemize}
  \item Clojure uses a \emph{hash array mapped trie} (HAMT) to
    implement persistent maps.
  \item A HAMT is a...
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Clojure's PersistentHashMap description}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Initial implementation based on ezyang's work}
  TODO: Find real initial implementation
  \begin{lstlisting}
data HashMap k v
    = Empty
    | BitmapIndexed {-# UNPACK #-} !Bitmap
                    {-# UNPACK #-} !(Array (HashMap k v))
    | Leaf {-# UNPACK #-} !(Leaf k v)
    | Full {-# UNPACK #-} !(Array (HashMap k v))
    | Collision {-# UNPACK #-} !Hash
                {-# UNPACK #-} !(Array (Leaf k v))

type Bitmap = Word
  \end{lstlisting}
\end{frame}

\begin{frame}
  \frametitle{Benchmarks}
\end{frame}

\begin{frame}
  \frametitle{Where is all the time spent?}
  Profiling
\end{frame}

\begin{frame}
  \frametitle{A better array copy}
\end{frame}

\begin{frame}
  \frametitle{Benchmarks}
\end{frame}

\begin{frame}
  \frametitle{Possible improvements}
  arraycopy, parallelism
\end{frame}

\begin{frame}
  \frametitle{Aside: the need for a sane hierarchy}
\end{frame}

\begin{frame}
  \frametitle{Summary}
  (Everything I said applies to sets as well)
\end{frame}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% TeX-PDF-mode: t
%%% End:
